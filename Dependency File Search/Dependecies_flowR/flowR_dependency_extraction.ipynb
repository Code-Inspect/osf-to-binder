{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "You can run the server locally using the following command:\n",
    "\n",
    "docker run -p 1042:1042 -it --rm -v /Users/lorrainesaju/flowR:/data eagleoutice/flowr --server\n",
    "\n",
    "(Here, \"/Users/lorrainesaju/flowR\" is the directory I have chosen to mount to the container. Replace it with the directory you want to use.)"
   ],
   "id": "9f84344bc5e0080"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Trying to replicate CLI behavior using the server. (QUERY) Using :query* gives JSON output. The below script takes in a project folder, in this case \"/Users/lorrainesaju/flowR\", and generates a dependencies file \"dependencies.txt\" with the libraries, sourced files, read data, and written data for all R files within the project. ",
   "id": "65f03059880d3c72"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T14:47:24.170109Z",
     "start_time": "2024-11-27T14:47:19.605680Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "def parse_flowr_output(raw_output):\n",
    "    \"\"\"Parses the raw output from flowR to extract dependencies.\"\"\"\n",
    "    if \"exit\" in raw_output:\n",
    "        raw_output = raw_output.split(\"exit\", 1)[1].strip()\n",
    "\n",
    "    # Extract JSON part\n",
    "    json_match = re.search(r'({.*})', raw_output, re.DOTALL)\n",
    "    if not json_match:\n",
    "        print(\"No valid JSON found in the output.\")\n",
    "        return None\n",
    "\n",
    "    json_str = json_match.group(1)\n",
    "\n",
    "    # Parse JSON\n",
    "    try:\n",
    "        dependencies = json.loads(json_str)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Extract relevant data\n",
    "    result = {\n",
    "        \"libraries\": [\n",
    "            lib[\"libraryName\"] for lib in dependencies.get(\"dependencies\", {}).get(\"libraries\", [])\n",
    "        ],\n",
    "        \"sourcedFiles\": [\n",
    "            file[\"file\"] for file in dependencies.get(\"dependencies\", {}).get(\"sourcedFiles\", [])\n",
    "        ],\n",
    "        \"readData\": [\n",
    "            data[\"source\"] for data in dependencies.get(\"dependencies\", {}).get(\"readData\", [])\n",
    "        ],\n",
    "        \"writtenData\": [\n",
    "            data[\"destination\"] for data in dependencies.get(\"dependencies\", {}).get(\"writtenData\", [])\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    return result\n",
    "\n",
    "def run_docker_flowr(query, file_path):\n",
    "    \"\"\"Runs the Docker flowR query for a given R file.\"\"\"\n",
    "    docker_command = [\n",
    "        \"docker\", \"run\", \"-i\", \"--rm\",\n",
    "        \"-v\", \"/Users/lorrainesaju/flowR:/data\",\n",
    "        \"eagleoutice/flowr\"\n",
    "    ]\n",
    "\n",
    "    # Adjust file path for the Docker container\n",
    "    container_file_path = f\"/data/{file_path}\"\n",
    "\n",
    "    query_command = f':query* \"[{{ \\\\\"type\\\\\": \\\\\"{query}\\\\\" }}]\" file://{container_file_path}'\n",
    "    try:\n",
    "        process = subprocess.Popen(\n",
    "            docker_command,\n",
    "            stdin=subprocess.PIPE,\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            text=True\n",
    "        )\n",
    "        stdout, stderr = process.communicate(input=f\"{query_command}\\nexit\\n\")\n",
    "        if stdout:\n",
    "            # print(f\"Result for {container_file_path}: {stdout}\")\n",
    "            return stdout\n",
    "        if stderr:\n",
    "            print(f\"Error: {stderr}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error running Docker command: {e}\")\n",
    "        return None\n",
    "\n",
    "def aggregate_dependencies(project_path):\n",
    "    \"\"\"Aggregates dependencies across all R files in a project.\"\"\"\n",
    "    dependencies = {\"libraries\": set(), \"sourcedFiles\": set(), \"readData\": set(), \"writtenData\": set()}\n",
    "    for root, _, files in os.walk(project_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".R\"):\n",
    "                # Adjust file path relative to the project root\n",
    "                relative_file_path = os.path.relpath(os.path.join(root, file), project_path)\n",
    "                print(f\"Processing {relative_file_path}...\")\n",
    "                raw_output = run_docker_flowr(\"dependencies\", relative_file_path)\n",
    "                if raw_output:\n",
    "                    parsed_deps = parse_flowr_output(raw_output)\n",
    "                    if parsed_deps:\n",
    "                        dependencies[\"libraries\"].update(parsed_deps[\"libraries\"])\n",
    "                        dependencies[\"sourcedFiles\"].update(parsed_deps[\"sourcedFiles\"])\n",
    "                        dependencies[\"readData\"].update(parsed_deps[\"readData\"])\n",
    "                        dependencies[\"writtenData\"].update(parsed_deps[\"writtenData\"])\n",
    "    return dependencies\n",
    "\n",
    "def generate_requirements_file(dependencies, output_file=\"dependencies.txt\"):\n",
    "    \"\"\"Generates a dependencies file for a project.\"\"\"\n",
    "    if not dependencies:\n",
    "        print(\"No dependencies to write.\")\n",
    "        return\n",
    "\n",
    "    with open(output_file, \"w\") as f:\n",
    "        f.write(\"# R libraries\\n\")\n",
    "        for library in sorted(dependencies[\"libraries\"]):\n",
    "            f.write(f\"{library}\\n\")\n",
    "\n",
    "        if dependencies[\"sourcedFiles\"]:\n",
    "            f.write(\"\\n# Sourced files\\n\")\n",
    "            for file in sorted(dependencies[\"sourcedFiles\"]):\n",
    "                f.write(f\"{file}\\n\")\n",
    "\n",
    "        if dependencies[\"readData\"]:\n",
    "            f.write(\"\\n# Data read\\n\")\n",
    "            for data in sorted(dependencies[\"readData\"]):\n",
    "                f.write(f\"{data}\\n\")\n",
    "\n",
    "        if dependencies[\"writtenData\"]:\n",
    "            f.write(\"\\n# Data written\\n\")\n",
    "            for data in sorted(dependencies[\"writtenData\"]):\n",
    "                f.write(f\"{data}\\n\")\n",
    "\n",
    "    print(f\"Dependencies file created: {output_file}\")\n",
    "\n",
    "# Main execution flow\n",
    "def process_project(project_path, output_file=\"dependencies.txt\"):\n",
    "    \"\"\"Processes a single project to generate a dependencies file.\"\"\"\n",
    "    dependencies = aggregate_dependencies(project_path)\n",
    "    generate_requirements_file(dependencies, output_file)\n",
    "\n",
    "# Example: Process a single project directory\n",
    "project_path = \"/Users/lorrainesaju/flowR\"\n",
    "output_file = os.path.join(project_path, \"dependencies.txt\")\n",
    "process_project(project_path, output_file)\n"
   ],
   "id": "f75cbc79e19ca1c6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test_file.R...\n",
      "parsed deps for test_file.R: {'libraries': ['ggplot2'], 'sourcedFiles': [], 'readData': [], 'writtenData': []}\n",
      "dependencies: {'libraries': set(), 'sourcedFiles': set(), 'readData': set(), 'writtenData': set()}\n",
      "dependencies updated: {'libraries': {'ggplot2'}, 'sourcedFiles': set(), 'readData': set(), 'writtenData': set()}\n",
      "Processing example_2.R...\n",
      "parsed deps for example_2.R: {'libraries': ['x'], 'sourcedFiles': [], 'readData': [], 'writtenData': []}\n",
      "dependencies: {'libraries': {'ggplot2'}, 'sourcedFiles': set(), 'readData': set(), 'writtenData': set()}\n",
      "dependencies updated: {'libraries': {'x', 'ggplot2'}, 'sourcedFiles': set(), 'readData': set(), 'writtenData': set()}\n",
      "Processing example_script.R...\n",
      "parsed deps for example_script.R: {'libraries': ['bar', 'better'], 'sourcedFiles': ['sample.R'], 'readData': ['data.csv'], 'writtenData': ['data2.csv', 'stdout']}\n",
      "dependencies: {'libraries': {'x', 'ggplot2'}, 'sourcedFiles': set(), 'readData': set(), 'writtenData': set()}\n",
      "dependencies updated: {'libraries': {'x', 'bar', 'better', 'ggplot2'}, 'sourcedFiles': {'sample.R'}, 'readData': {'data.csv'}, 'writtenData': {'data2.csv', 'stdout'}}\n",
      "Final dependencies: {'libraries': {'x', 'bar', 'better', 'ggplot2'}, 'sourcedFiles': {'sample.R'}, 'readData': {'data.csv'}, 'writtenData': {'data2.csv', 'stdout'}}\n",
      "Dependencies file created: /Users/lorrainesaju/flowR/dependencies.txt\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ddb6306fa41df321"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
